# Main Configuration for LLM Influence Analysis & Evaluation
# ==============================================

# Model Configuration
models:
  base:
    name: "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"
    id: "base_tinyllama"
  finetuned:
    path: "./tinyllama_1b_model"
    id: "finetuned_tinyllama"

# Dataset Configuration
dataset:
  name: "Trelis/big_patent_sample"
  num_samples: 10000

# General Configuration
general:
  max_length: 2048 
  prompts_file: "prompts.json"
  seed: 42
  use_flash_attention: true

# Output Directories
output:
  influence_results: "influence_results/factor_analysis"
  comparison_results: "comparison_results"
  olmes_results: "olmes_results"
  combined_results: "combined_evaluation_results"

# Factor Analysis Configuration
factors:
  name: "tinyllama_1b_factors"
  all_layers_name: "tinyllama_1b_factors_all_layers"
  strategy: "ekfac"
  batch_size: 8
  num_workers: 8

# Score Computation Configuration
scores:
  name: "tinyllama_prompt_scores"
  all_layers_name: "tinyllama_prompt_scores_all_layers"
  generated_name: "tinyllama_generated_scores"
  query_gradient_rank: 64
  train_batch_size: 4
  num_influential: 10

# Evaluation Configuration
evaluation:
  generated_answers_file: "generated_answers.json"
  finetuned_answers_file: "finetuned_generated_answers.json"
  
  # OLMES Benchmark Configuration
  olmes:
    # Path to OLMES installation
    dir: "../olmes"
    limit_examples: 50
    tasks:
      - task_name: "arc_challenge"
        description: "AI2 Reasoning Challenge (25-shot)"
        type: "multiple_choice"
        shots: 25
        data_source: "data:ai2_arc/test?subset=Challenge"
      
      - task_name: "mmlu_humanities"
        description: "MMLU Humanities (5-shot)"
        type: "multiple_choice"
        shots: 5
        data_source: "data:cais_mmlu/test?subset=humanities"
      
      - task_name: "mmlu_stem"
        description: "MMLU STEM (5-shot)"
        type: "multiple_choice"
        shots: 5
        data_source: "data:cais_mmlu/test?subset=stem"
      
      - task_name: "gsm8k"
        description: "Grade School Math 8K (5-shot)"
        type: "generation"
        shots: 5
        data_source: "data:gsm8k/test"
        metrics:
          - name: "accuracy"
      
      - task_name: "truthful_qa"
        description: "TruthfulQA Multiple Choice (0-shot)"
        type: "multiple_choice"
        shots: 0
        data_source: "data:truthful_qa/generation" 